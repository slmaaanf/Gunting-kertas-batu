{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Muhammad Fauzul Hanif\n",
        "Email : muhammadfauzulhanif16@gmail.com\n",
        "Alamat : Sukabumi"
      ],
      "metadata": {
        "id": "PipjBr5ZB9tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdtmyXPxNC8i",
        "outputId": "b5053512-c232-4003-ef6d-05e8ff5156b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-23 02:21:35--  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/391417272/7eb836f2-695b-4a46-9c78-b65867166957?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240823%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240823T022135Z&X-Amz-Expires=300&X-Amz-Signature=1c5c651ea0d33ca63b238898efc2026f4971a1a83b55dfbbf1c841b63c74314c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=391417272&response-content-disposition=attachment%3B%20filename%3Drockpaperscissors.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-08-23 02:21:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/391417272/7eb836f2-695b-4a46-9c78-b65867166957?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240823%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240823T022135Z&X-Amz-Expires=300&X-Amz-Signature=1c5c651ea0d33ca63b238898efc2026f4971a1a83b55dfbbf1c841b63c74314c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=391417272&response-content-disposition=attachment%3B%20filename%3Drockpaperscissors.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322873683 (308M) [application/octet-stream]\n",
            "Saving to: ‘rockpaperscissors.zip’\n",
            "\n",
            "rockpaperscissors.z 100%[===================>] 307.92M  47.8MB/s    in 5.1s    \n",
            "\n",
            "2024-08-23 02:21:41 (60.4 MB/s) - ‘rockpaperscissors.zip’ saved [322873683/322873683]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip\n",
        "!unzip -q rockpaperscissors.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v51AQeSLL4Oa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "base_dir = 'rockpaperscissors'\n",
        "categories = ['rock', 'paper', 'scissors']\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(validation_dir, category), exist_ok=True)\n",
        "\n",
        "    category_dir = os.path.join(base_dir, category)\n",
        "    images = os.listdir(category_dir)\n",
        "    train_images, validation_images = train_test_split(images, test_size=0.4)\n",
        "\n",
        "    for image in train_images:\n",
        "        shutil.move(os.path.join(category_dir, image), os.path.join(train_dir, category))\n",
        "\n",
        "    for image in validation_images:\n",
        "        shutil.move(os.path.join(category_dir, image), os.path.join(validation_dir, category))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3f4fblMWGQ",
        "outputId": "2bfc3b35-0593-446d-ea7b-df2c29ce8ef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1312 images belonging to 3 classes.\n",
            "Found 876 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bElDVOw3MZmf",
        "outputId": "146252ae-620d-43fe-e422-4ac8c242a107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_20bfBqMm8F",
        "outputId": "3bcdd9c4-2d21-4eca-8d3a-720297625e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 3s/step - accuracy: 0.3968 - loss: 1.2519 - val_accuracy: 0.6998 - val_loss: 0.7078\n",
            "Epoch 2/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.7341 - loss: 0.6368 - val_accuracy: 0.9064 - val_loss: 0.2736\n",
            "Epoch 3/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.8819 - loss: 0.3308 - val_accuracy: 0.9600 - val_loss: 0.1459\n",
            "Epoch 4/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9162 - loss: 0.2322 - val_accuracy: 0.8733 - val_loss: 0.3392\n",
            "Epoch 5/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 3s/step - accuracy: 0.9313 - loss: 0.2210 - val_accuracy: 0.9692 - val_loss: 0.1015\n",
            "Epoch 6/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9358 - loss: 0.1533 - val_accuracy: 0.9806 - val_loss: 0.0622\n",
            "Epoch 7/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9727 - loss: 0.0931 - val_accuracy: 0.9795 - val_loss: 0.0651\n",
            "Epoch 8/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9796 - loss: 0.0821 - val_accuracy: 0.9600 - val_loss: 0.1109\n",
            "Epoch 9/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9567 - loss: 0.1237 - val_accuracy: 0.9795 - val_loss: 0.0631\n",
            "Epoch 10/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.9764 - loss: 0.0895 - val_accuracy: 0.9840 - val_loss: 0.0610\n",
            "Epoch 11/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - accuracy: 0.9647 - loss: 0.1139 - val_accuracy: 0.9749 - val_loss: 0.0846\n",
            "Epoch 12/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 2s/step - accuracy: 0.9842 - loss: 0.0522 - val_accuracy: 0.9852 - val_loss: 0.0514\n",
            "Epoch 13/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.9879 - loss: 0.0442 - val_accuracy: 0.9817 - val_loss: 0.0598\n",
            "Epoch 14/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.9839 - loss: 0.0581 - val_accuracy: 0.9817 - val_loss: 0.0497\n",
            "Epoch 15/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 3s/step - accuracy: 0.9901 - loss: 0.0333 - val_accuracy: 0.9874 - val_loss: 0.0446\n",
            "Epoch 16/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.9902 - loss: 0.0300 - val_accuracy: 0.9658 - val_loss: 0.1149\n",
            "Epoch 17/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 3s/step - accuracy: 0.9787 - loss: 0.0606 - val_accuracy: 0.9863 - val_loss: 0.0515\n",
            "Epoch 18/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - accuracy: 0.9831 - loss: 0.0472 - val_accuracy: 0.9806 - val_loss: 0.0783\n",
            "Epoch 19/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.9891 - loss: 0.0472 - val_accuracy: 0.9829 - val_loss: 0.0552\n",
            "Epoch 20/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9845 - loss: 0.0398 - val_accuracy: 0.9806 - val_loss: 0.0690\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDbW0K7dMpsV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "\n",
        "model = tf.keras.models.load_model('rps_model.h5')\n",
        "\n",
        "def predict_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(150, 150))\n",
        "    img_tensor = image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "    img_tensor /= 255.\n",
        "\n",
        "    prediction = model.predict(img_tensor)\n",
        "    class_index = np.argmax(prediction)\n",
        "    class_labels = ['paper', 'rock', 'scissors']\n",
        "    predicted_label = class_labels[class_index]\n",
        "\n",
        "    plt.imshow(mpimg.imread(img_path))\n",
        "    plt.title(f\"Prediction: {predicted_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def upload_and_predict():\n",
        "    while True:\n",
        "        uploaded = files.upload()\n",
        "        for fn in uploaded.keys():\n",
        "            predict_image(fn)\n",
        "        cont = input(\"Do you want to upload another image? (yes/no): \").strip().lower()\n",
        "        if cont != 'yes':\n",
        "            break\n",
        "\n",
        "upload_and_predict()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}